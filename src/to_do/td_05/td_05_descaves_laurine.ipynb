{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "597Hu82gSNCZ"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
    "\n",
    "# Traitement de donn√©es & Programmation en Python\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-XK862WiMt"
   },
   "source": [
    "# TD 05\n",
    "\n",
    "Traitement de donn√©es\n",
    "\n",
    "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
    " \n",
    "Elements √† consulter:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Doc                                   |             Link\n",
    "--------------------------------------|------------------------------------\n",
    "Github Helper      | [>link<](#scrollTo=Github_101)\n",
    "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
    "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1g4EgT41MqJ"
   },
   "source": [
    "## Intro\n",
    "\n",
    "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjiE8c51VoT0",
    "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is awesome üëç\n"
     ]
    }
   ],
   "source": [
    "# Installs\n",
    "print(\"Python is awesome üëç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "JhT-50uwz72F"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsB9ADLg8i8P",
    "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKow2OZ5LCXr"
   },
   "source": [
    "### Correction TD precedent\n",
    "\n",
    "Partir d'un fichier csv et alimenter la binge watch list :\n",
    "  > serie, plateform, nb_episodes, ann√©e_sortie, note\n",
    "\n",
    "Puis print :\n",
    "\n",
    "\t Nom de la s√©rie : Brooklyn Nine-Nine\n",
    "\t Ann√©e de sortie : 2013\n",
    "\n",
    "\t Nom de la s√©rie : The office\n",
    "\t Ann√©e de sortie : 2005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9lcQQFp3w0HE"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './netflix_titles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1400\\2136715073.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# file_path = \"./tv_shows_pipe.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mbinge_watch_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m# for serie in binge_watch_list:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#   print(serie.get('title'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1400\\2136715073.py\u001b[0m in \u001b[0;36mcsv_to_dict\u001b[1;34m(file_path, separator, row_limit)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mserie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mserie_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './netflix_titles.csv'"
     ]
    }
   ],
   "source": [
    "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
    "    \"\"\"\n",
    "    Transforms a csv file, into a list of dictionaries\n",
    "\n",
    "    Arguments:\n",
    "        file_path  {str} : eg. /documents/file.txt\n",
    "        separator  {str} : separator of fields, default (;)\n",
    "        row_limit  {int} : limit the rows to be read, default None\n",
    "\n",
    "    Returns list[dict]\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        serie = {}\n",
    "        serie_list = []\n",
    "        for i, row in enumerate(f, 1):\n",
    "            if i == 1:\n",
    "                header = row.lower().strip().replace(' ','_').split(separator)\n",
    "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
    "            else:\n",
    "                data = row.lower().strip().split(separator)\n",
    "                # print(\"Data:\",data)\n",
    "                if len(header) == len(data):\n",
    "                    for j, element in enumerate(header):\n",
    "                        serie[header[j]] = data[j]\n",
    "            # print(serie)\n",
    "                serie_list.append(serie)\n",
    "                serie = {}\n",
    "            if i == row_limit:\n",
    "                break\n",
    "    return serie_list\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"./netflix_titles.csv\"\n",
    "# file_path = \"./tv_shows_pipe.csv\"\n",
    "limit = None\n",
    "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
    "# for serie in binge_watch_list:\n",
    "#   print(serie.get('title'))\n",
    "# # Option 1 read some rows to see what we have\n",
    "\n",
    "df = pd.DataFrame(binge_watch_list)\n",
    "df[\"title\"] = df[\"title\"].str.title()\n",
    "df.head(5)\n",
    "\n",
    "column_name = 'release_year'\n",
    "filter = df['release_year'] == '2005'\n",
    "# df.query(f\"`{column_name}` == '2003'\")\n",
    "df[filter]\n",
    "# # serie, plateform, nb_episodes, ann√©e_sortie, note\n",
    "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
    "#               var_name='plateform', \n",
    "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
    "# df2.drop(['value'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2lJWv62kQZA"
   },
   "source": [
    "##EX01\n",
    "### Numpy intro\n",
    "\n",
    "* Cr√©er un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
    "    * Assigner le r√©sultat √† la variable `villes`\n",
    "* Cr√©er une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
    "    * Assigner le r√©sultat √† la variable `villes_departement`\n",
    "* Assigner la taille du vecteur `villes` √† la variable `v_shape`\n",
    "* Assigner la taille du tableau `villes_departement` √† la variable `vd_shape` \n",
    "* Afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n_jOsgGllUhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lyon' 'Paris' 'Montpellier']\n",
      "(3, 2)\n",
      "(3,)\n",
      "[['Lyon' '69']\n",
      " ['Paris' '75']\n",
      " ['Montpellier' '34']]\n"
     ]
    }
   ],
   "source": [
    "villes = np.array(['Lyon', 'Paris', 'Montpellier'])\n",
    "print(villes)\n",
    "a = np.mat([['Lyon', '69'],\n",
    "            ['Paris', '75'],\n",
    "            ['Montpellier', '34']])\n",
    "villes_departement = a \n",
    "print(a.shape)\n",
    "\n",
    "v_shape = villes.shape\n",
    "\n",
    "\n",
    "print(v_shape)\n",
    "vd_shape = villes_departement\n",
    "\n",
    "print(vd_shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8WBS9oVKcWs"
   },
   "source": [
    "##EX02\n",
    "### Numpy intro\n",
    "\n",
    "#### Part 1\n",
    "\n",
    "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
    "\n",
    "```\n",
    "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
    "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
    "'rating']\n",
    "```\n",
    "\n",
    "* Afficher les deux premieres colonnes et toute les lignes\n",
    "* Afficher la derni√®re colonne\n",
    "* Afficher les 5 premieres lignes de la 4√®me colonne\n",
    "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes \n",
    "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
    "* Mettre dans une liste toutes les marques de c√©reales en utilisant `.tolist()` & afficher cette derni√®re\n",
    "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette derni√®re\n",
    "> bonus \n",
    "* Convertir la colonne ratings en d√©cimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTDsqYIgpitx",
    "outputId": "d2cd6d75-cf4d-4421-c34f-097def1f3b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['name' 'mfr']\n",
      " ['100% Bran' 'N']\n",
      " ['100% Natural Bran' 'Q']\n",
      " ['All-Bran' 'K']\n",
      " ['All-Bran with Extra Fiber' 'K']\n",
      " ['Almond Delight' 'R']\n",
      " ['Apple Cinnamon Cheerios' 'G']\n",
      " ['Apple Jacks' 'K']\n",
      " ['Basic 4' 'G']\n",
      " ['Bran Chex' 'R']\n",
      " ['Bran Flakes' 'P']\n",
      " [\"Cap'n'Crunch\" 'Q']\n",
      " ['Cheerios' 'G']\n",
      " ['Cinnamon Toast Crunch' 'G']\n",
      " ['Clusters' 'G']\n",
      " ['Cocoa Puffs' 'G']\n",
      " ['Corn Chex' 'R']\n",
      " ['Corn Flakes' 'K']\n",
      " ['Corn Pops' 'K']\n",
      " ['Count Chocula' 'G']\n",
      " [\"Cracklin' Oat Bran\" 'K']\n",
      " ['Cream of Wheat (Quick)' 'N']\n",
      " ['Crispix' 'K']\n",
      " ['Crispy Wheat & Raisins' 'G']\n",
      " ['Double Chex' 'R']\n",
      " ['Froot Loops' 'K']\n",
      " ['Frosted Flakes' 'K']\n",
      " ['Frosted Mini-Wheats' 'K']\n",
      " ['Fruit & Fibre Dates; Walnuts; and Oats' 'P']\n",
      " ['Fruitful Bran' 'K']\n",
      " ['Fruity Pebbles' 'P']\n",
      " ['Golden Crisp' 'P']\n",
      " ['Golden Grahams' 'G']\n",
      " ['Grape Nuts Flakes' 'P']\n",
      " ['Grape-Nuts' 'P']\n",
      " ['Great Grains Pecan' 'P']\n",
      " ['Honey Graham Ohs' 'Q']\n",
      " ['Honey Nut Cheerios' 'G']\n",
      " ['Honey-comb' 'P']\n",
      " ['Just Right Crunchy  Nuggets' 'K']\n",
      " ['Just Right Fruit & Nut' 'K']\n",
      " ['Kix' 'G']\n",
      " ['Life' 'Q']\n",
      " ['Lucky Charms' 'G']\n",
      " ['Maypo' 'A']\n",
      " ['Muesli Raisins; Dates; & Almonds' 'R']\n",
      " ['Muesli Raisins; Peaches; & Pecans' 'R']\n",
      " ['Mueslix Crispy Blend' 'K']\n",
      " ['Multi-Grain Cheerios' 'G']\n",
      " ['Nut&Honey Crunch' 'K']\n",
      " ['Nutri-Grain Almond-Raisin' 'K']\n",
      " ['Nutri-grain Wheat' 'K']\n",
      " ['Oatmeal Raisin Crisp' 'G']\n",
      " ['Post Nat. Raisin Bran' 'P']\n",
      " ['Product 19' 'K']\n",
      " ['Puffed Rice' 'Q']\n",
      " ['Puffed Wheat' 'Q']\n",
      " ['Quaker Oat Squares' 'Q']\n",
      " ['Quaker Oatmeal' 'Q']\n",
      " ['Raisin Bran' 'K']\n",
      " ['Raisin Nut Bran' 'G']\n",
      " ['Raisin Squares' 'K']\n",
      " ['Rice Chex' 'R']\n",
      " ['Rice Krispies' 'K']\n",
      " ['Shredded Wheat' 'N']\n",
      " [\"Shredded Wheat 'n'Bran\" 'N']\n",
      " ['Shredded Wheat spoon size' 'N']\n",
      " ['Smacks' 'K']\n",
      " ['Special K' 'K']\n",
      " ['Strawberry Fruit Wheats' 'N']\n",
      " ['Total Corn Flakes' 'G']\n",
      " ['Total Raisin Bran' 'G']\n",
      " ['Total Whole Grain' 'G']\n",
      " ['Triples' 'G']\n",
      " ['Trix' 'G']\n",
      " ['Wheat Chex' 'R']\n",
      " ['Wheaties' 'G']\n",
      " ['Wheaties Honey Gold' 'G']]\n",
      "rating\n",
      "['calories' '70' '120' '70' '50']\n",
      "['100']\n",
      "All-Bran\n",
      "['name' '100% Bran' '100% Natural Bran' 'All-Bran'\n",
      " 'All-Bran with Extra Fiber' 'Almond Delight' 'Apple Cinnamon Cheerios'\n",
      " 'Apple Jacks' 'Basic 4' 'Bran Chex' 'Bran Flakes' \"Cap'n'Crunch\"\n",
      " 'Cheerios' 'Cinnamon Toast Crunch' 'Clusters' 'Cocoa Puffs' 'Corn Chex'\n",
      " 'Corn Flakes' 'Corn Pops' 'Count Chocula' \"Cracklin' Oat Bran\"\n",
      " 'Cream of Wheat (Quick)' 'Crispix' 'Crispy Wheat & Raisins' 'Double Chex'\n",
      " 'Froot Loops' 'Frosted Flakes' 'Frosted Mini-Wheats'\n",
      " 'Fruit & Fibre Dates; Walnuts; and Oats' 'Fruitful Bran' 'Fruity Pebbles'\n",
      " 'Golden Crisp' 'Golden Grahams' 'Grape Nuts Flakes' 'Grape-Nuts'\n",
      " 'Great Grains Pecan' 'Honey Graham Ohs' 'Honey Nut Cheerios' 'Honey-comb'\n",
      " 'Just Right Crunchy  Nuggets' 'Just Right Fruit & Nut' 'Kix' 'Life'\n",
      " 'Lucky Charms' 'Maypo' 'Muesli Raisins; Dates; & Almonds'\n",
      " 'Muesli Raisins; Peaches; & Pecans' 'Mueslix Crispy Blend'\n",
      " 'Multi-Grain Cheerios' 'Nut&Honey Crunch' 'Nutri-Grain Almond-Raisin'\n",
      " 'Nutri-grain Wheat' 'Oatmeal Raisin Crisp' 'Post Nat. Raisin Bran'\n",
      " 'Product 19' 'Puffed Rice' 'Puffed Wheat' 'Quaker Oat Squares'\n",
      " 'Quaker Oatmeal' 'Raisin Bran' 'Raisin Nut Bran' 'Raisin Squares'\n",
      " 'Rice Chex' 'Rice Krispies' 'Shredded Wheat' \"Shredded Wheat 'n'Bran\"\n",
      " 'Shredded Wheat spoon size' 'Smacks' 'Special K'\n",
      " 'Strawberry Fruit Wheats' 'Total Corn Flakes' 'Total Raisin Bran'\n",
      " 'Total Whole Grain' 'Triples' 'Trix' 'Wheat Chex' 'Wheaties'\n",
      " 'Wheaties Honey Gold']\n",
      "['100% Bran' '100% Natural Bran' 'All-Bran' 'All-Bran with Extra Fiber'\n",
      " 'Almond Delight' 'Apple Cinnamon Cheerios' 'Apple Jacks' 'Basic 4'\n",
      " 'Bran Chex' 'Bran Flakes']\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "file_path = \"C:/Users/Laurine D/m1-miage/src/to_do/td_05/cereal.csv\"\n",
    "data = np.genfromtxt(file_path, delimiter = \",\", dtype=str)\n",
    "\n",
    "x = np.array(data)\n",
    "cereales = data[0]\n",
    "#print(cereales)\n",
    "#type(x)\n",
    "\n",
    "## retourne (lignes, colonnes)\n",
    "#print(data.shape)\n",
    "\n",
    "#afficher les deux premi√®res colonnes et toutes les lignes \n",
    "y = data[:, :2]\n",
    "print(y)\n",
    "\n",
    "#afficher la derni√®re colonne\n",
    "colonne = data[0,-1]\n",
    "print(colonne)\n",
    "\n",
    "#afficher les 5 premi√®res lignes de la 4√®me colonne\n",
    "\n",
    "fiverows = data[:5, 3]\n",
    "print(fiverows)\n",
    "\n",
    "\n",
    "#Assigner a une variable corn_flakes_cals le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes\n",
    "\n",
    "corn = ( data[:,0] == 'Corn Flakes')\n",
    "corn_flakes_cals = data[corn,3]\n",
    "print(corn_flakes_cals)\n",
    "    \n",
    "#Assigner le nom de la 3eme marque sur le dataset a une variabe third_brand\n",
    "third_brand = data[3, 0]\n",
    "print(third_brand)\n",
    "\n",
    "#Mettre dans une liste toutes les marques de c√©reales en utilisant .tolist() & afficher cette derni√®re\n",
    "liste = data[0:,0]\n",
    "liste_cere = np.array(liste.tolist())\n",
    "print(liste_cere)\n",
    "\n",
    "#Mettre les 10 premiere marques dans le fichiers dans une liste first_ten_brands & afficher cette derni√®re\n",
    "liste_10 = data[1:11, 0]\n",
    "first_ten_brands = np.array(liste_10.tolist())\n",
    "print(first_ten_brands)\n",
    "\n",
    "#Convertir la colonne ratings en d√©cimale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BaW1VvWXCS7"
   },
   "source": [
    "#### Part 2\n",
    "\n",
    "* **Extraire** la premiere colonne du dataset et **la comparer** √† la valeur `'Special K'` Assigner le resultat √† une variable `special_k`\n",
    "* Executer la commande `print(data[special_k])`\n",
    "> bonus\n",
    "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset üòâ)\n",
    "> bonus ¬≤\n",
    "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
    "* Ajouter une marque de c√©r√©ales \"healthy\" avec 0 grammes de sucre au dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuXFyhUepnMc",
    "outputId": "6ec9c05d-a76c-4d8f-8c48-6285d32b6953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Special K' 'K' 'C' '110' '6' '0' '230' '1' '16' '3' '55' '25' '1' '1'\n",
      "  '1' '53.131324']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'numpy.ndarray' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17036\\970018936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmanufacturer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mfr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mquaker_oats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Q\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msugars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquaker_oats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msugars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'numpy.ndarray' and 'int'"
     ]
    }
   ],
   "source": [
    "special_k = (data[:,0] == 'Special K')\n",
    "\n",
    "print(data[special_k])\n",
    "\n",
    "manufacturer = (data[0,:] == 'mfr')\n",
    "quaker_oats = (data[:,1] == \"Q\")\n",
    "sugars = len(data[:,9] < 10)\n",
    "\n",
    "print((data[quaker_oats, sugars]))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdsY64gl_0dw"
   },
   "source": [
    "#### Part 3\n",
    "\n",
    "Sur le m√™me dataset, √† l'aide d'une boucle `for`:\n",
    "\n",
    "\n",
    "*   Cr√©er un dictionnaire avec la moyenne de sucre par Fabricant de c√©rales\n",
    "*   Trouver la marque de c√©reales qui contient le plus de sucre par portion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjbTgYNbCaIj",
    "outputId": "5ab1f623-f8df-45f2-e82b-7031da73f3f5"
   },
   "outputs": [],
   "source": [
    "fabricant = data[0,:] == 'mfr'\n",
    "\n",
    "for i in fabricant:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1wjqra3_yZo",
    "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17036\\1974426928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mportion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\42\\Python\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "portion = data\n",
    "maximum = data[:,9]\n",
    "print(maximum.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHsrGdy-5AZK"
   },
   "source": [
    "## EX03\n",
    "### Dataframes\n",
    "\n",
    "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "jIatRWhm5Xzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nom': ('ID,Title,Year,Age,IMDb,Rotten Tomatoes,Netflix,Hulu,Prime Video,Disney+,Type;',), 'Netflix': ('5,6,Avatar: The Last Airbender,2005,7+,9.3/10,93/100,1,0,1,0,1;',), 'Prime Videos': ('7,8,The Walking Dead,2010,18+,8.2/10,93/100,1,0,0,0,1;',), 'Disney+': ('8,9,Black Mirror,2011,18+,8.8/10,92/100,1,0,0,0,1;',), 'Hulu': ('6,7,Peaky Blinders,2013,18+,8.8/10,93/100,1,0,0,0,1;',), 'Ann√©e': ('1,2,Stranger Things,2016,16+,8.7/10,96/100,1,0,0,0,1;',), 'Note IMDB': ('3,4,Better Call Saul,2015,18+,8.8/10,94/100,1,0,0,0,1;',), 'Note RottenTomatoes': ('4,5,Dark,2017,16+,8.8/10,93/100,1,0,0,0,1;',)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurine D\\AppData\\Local\\Temp\\ipykernel_17036\\20657183.py:2: ConversionWarning: Some errors were detected !\n",
      "    Line #3425 (got 2 columns instead of 1)\n",
      "    Line #3447 (got 2 columns instead of 1)\n",
      "  data_shows = np.genfromtxt(file_path, delimiter = \"|\", dtype='str', filling_values = None, encoding = \"utf-8\", invalid_raise=False)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Laurine D/dev/m1-miage/src/to_do/td_04/tv_shows.csv\"\n",
    "data_shows = np.genfromtxt(file_path, delimiter = \"|\", dtype='str', filling_values = None, encoding = \"utf-8\", invalid_raise=False)\n",
    "#pleins d'erreurs de colonnes \n",
    "\n",
    "\n",
    "parametres = data_shows[::]\n",
    "serie_list = []\n",
    "\n",
    "for data in parametres:\n",
    "    \n",
    "            data = \"\".join(data)\n",
    "           \n",
    "            titre = parametres[0],\n",
    "            plateforme = parametres[6],\n",
    "            plateforme2 = parametres[8],\n",
    "            plateforme3 = parametres[9],\n",
    "            plateforme4 = parametres[7],\n",
    "            annee = parametres[2],\n",
    "            note_imdb = parametres[4],\n",
    "            note_rottentomatoes = parametres[5],\n",
    "\n",
    "            \n",
    "            series = {\"Nom\":titre, \"Netflix\":plateforme, \"Prime Videos\":plateforme2, \"Disney+\":plateforme3, \"Hulu\":plateforme4, \"Ann√©e\":annee, \"Note IMDB\":note_imdb,     \"Note RottenTomatoes\":note_rottentomatoes}\n",
    "            serie_list.append(series)\n",
    "  \n",
    "             \n",
    "print(serie_list[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94-12vgPXG10"
   },
   "source": [
    "## Github 101\n",
    "\n",
    "```shell\n",
    "# Repartir sur une nouvelle branche\n",
    "git checkout main\n",
    "# Mettre √† jour notre branche locale\n",
    "git pull origin main\n",
    "# Recr√©er une branche pour le TD 05\n",
    "git checkout -b branch_name_td05\n",
    "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
    "cd 'chemin/m1-miage/'\n",
    "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
    "# Faire le travail necessaire sur votre fichier et commit et push\n",
    "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
    "git commit -m 'message descriptif du travail fait'\n",
    "git push --set-upstream origin nom_branche\n",
    "# Faire une pull request\n",
    "```\n",
    "\n",
    "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "597Hu82gSNCZ",
    "k2lJWv62kQZA",
    "94-12vgPXG10"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
