{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "597Hu82gSNCZ",
        "k2lJWv62kQZA",
        "94-12vgPXG10"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "597Hu82gSNCZ"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
        "\n",
        "# Traitement de donn√©es & Programmation en Python\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TD 05\n",
        "\n",
        "Traitement de donn√©es\n",
        "\n",
        "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
        " \n",
        "Elements √† consulter:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Doc                                   |             Link\n",
        "--------------------------------------|------------------------------------\n",
        "Github Helper      | [>link<](#scrollTo=Github_101)\n",
        "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
        "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
      ],
      "metadata": {
        "id": "EA-XK862WiMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "\n",
        "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
      ],
      "metadata": {
        "id": "Y1g4EgT41MqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "print(\"Python is awesome üëç\")"
      ],
      "metadata": {
        "id": "yjiE8c51VoT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is awesome üëç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JhT-50uwz72F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsB9ADLg8i8P",
        "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correction TD precedent\n",
        "\n",
        "Partir d'un fichier csv et alimenter la binge watch list :\n",
        "  > serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "\n",
        "Puis print :\n",
        "\n",
        "\t Nom de la s√©rie : Brooklyn Nine-Nine\n",
        "\t Ann√©e de sortie : 2013\n",
        "\n",
        "\t Nom de la s√©rie : The office\n",
        "\t Ann√©e de sortie : 2005\n"
      ],
      "metadata": {
        "id": "iKow2OZ5LCXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
        "    \"\"\"\n",
        "    Transforms a csv file, into a list of dictionaries\n",
        "\n",
        "    Arguments:\n",
        "        file_path  {str} : eg. /documents/file.txt\n",
        "        separator  {str} : separator of fields, default (;)\n",
        "        row_limit  {int} : limit the rows to be read, default None\n",
        "\n",
        "    Returns list[dict]\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        serie = {}\n",
        "        serie_list = []\n",
        "        for i, row in enumerate(f, 1):\n",
        "            if i == 1:\n",
        "                header = row.lower().strip().replace(' ','_').split(separator)\n",
        "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
        "            else:\n",
        "                data = row.lower().strip().split(separator)\n",
        "                # print(\"Data:\",data)\n",
        "                if len(header) == len(data):\n",
        "                    for j, element in enumerate(header):\n",
        "                        serie[header[j]] = data[j]\n",
        "            # print(serie)\n",
        "                serie_list.append(serie)\n",
        "                serie = {}\n",
        "            if i == row_limit:\n",
        "                break\n",
        "    return serie_list\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"./netflix_titles.csv\"\n",
        "# file_path = \"./tv_shows_pipe.csv\"\n",
        "limit = None\n",
        "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
        "# for serie in binge_watch_list:\n",
        "#   print(serie.get('title'))\n",
        "# # Option 1 read some rows to see what we have\n",
        "\n",
        "df = pd.DataFrame(binge_watch_list)\n",
        "df[\"title\"] = df[\"title\"].str.title()\n",
        "df.head(5)\n",
        "\n",
        "column_name = 'release_year'\n",
        "filter = df['release_year'] == '2005'\n",
        "# df.query(f\"`{column_name}` == '2003'\")\n",
        "df[filter]\n",
        "# # serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
        "#               var_name='plateform', \n",
        "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
        "# df2.drop(['value'], axis=1)"
      ],
      "metadata": {
        "id": "9lcQQFp3w0HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EX01\n",
        "### Numpy intro\n",
        "\n",
        "* Cr√©er un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
        "    * Assigner le r√©sultat √† la variable `villes`\n",
        "* Cr√©er une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
        "    * Assigner le r√©sultat √† la variable `villes_departement`\n",
        "* Assigner la taille du vecteur `villes` √† la variable `v_shape`\n",
        "* Assigner la taille du tableau `villes_departement` √† la variable `vd_shape` \n",
        "* Afficher les r√©sultats"
      ],
      "metadata": {
        "id": "k2lJWv62kQZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "n_jOsgGllUhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EX02\n",
        "### Numpy intro\n",
        "\n",
        "#### Part 1\n",
        "\n",
        "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
        "\n",
        "```\n",
        "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
        "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
        "'rating']\n",
        "```\n",
        "\n",
        "* Afficher les deux premieres colonnes et toute les lignes\n",
        "* Afficher la derni√®re colonne\n",
        "* Afficher les 5 premieres lignes de la 4√®me colonne\n",
        "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes \n",
        "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
        "* Mettre dans une liste toutes les marques de c√©reales en utilisant `.tolist()` & afficher cette derni√®re\n",
        "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette derni√®re\n",
        "> bonus \n",
        "* Convertir la colonne ratings en d√©cimale"
      ],
      "metadata": {
        "id": "Q8WBS9oVKcWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n",
        "file_path = \"./cereal.csv\"\n",
        "data = np.genfromtxt(\n",
        "                      # ....\n",
        "                     )\n",
        "print(data.shape) ## retourne (lignes, colonnes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTDsqYIgpitx",
        "outputId": "d2cd6d75-cf4d-4421-c34f-097def1f3b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(77, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2\n",
        "\n",
        "* **Extraire** la premiere colonne du dataset et **la comparer** √† la valeur `'Special K'` Assigner le resultat √† une variable `special_k`\n",
        "* Executer la commande `print(data[special_k])`\n",
        "> bonus\n",
        "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset üòâ)\n",
        "> bonus ¬≤\n",
        "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
        "* Ajouter une marque de c√©r√©ales \"healthy\" avec 0 grammes de sucre au dataset "
      ],
      "metadata": {
        "id": "8BaW1VvWXCS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuXFyhUepnMc",
        "outputId": "6ec9c05d-a76c-4d8f-8c48-6285d32b6953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Special K' 'K' 'C' '110' '6' '0' '230' '1' '16' '3' '55' '25' '1' '1'\n",
            "  '1' '53.131324']]\n",
            "['100% Natural Bran' 'Life' 'Puffed Rice' 'Puffed Wheat'\n",
            " 'Quaker Oat Squares' 'Quaker Oatmeal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3\n",
        "\n",
        "Sur le m√™me dataset, √† l'aide d'une boucle `for`:\n",
        "\n",
        "\n",
        "*   Cr√©er un dictionnaire avec la moyenne de sucre par Fabricant de c√©rales\n",
        "*   Trouver la marque de c√©reales qui contient le plus de sucre par portion\n",
        "\n"
      ],
      "metadata": {
        "id": "NdsY64gl_0dw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbTgYNbCaIj",
        "outputId": "5ab1f623-f8df-45f2-e82b-7031da73f3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['100% Bran', 'Nabisco', 'C', ..., '1', '0.33', '68.402973'],\n",
              "       ['100% Natural Bran', 'Quaker Oats', 'C', ..., '1', '1',\n",
              "        '33.983679'],\n",
              "       ['All-Bran', 'Kelloggs', 'C', ..., '1', '0.33', '59.425505'],\n",
              "       ...,\n",
              "       ['Wheat Chex', 'Ralston Purina', 'C', ..., '1', '0.67',\n",
              "        '49.787445'],\n",
              "       ['Wheaties', 'General Mills', 'C', ..., '1', '1', '51.592193'],\n",
              "       ['Wheaties Honey Gold', 'General Mills', 'C', ..., '1', '0.75',\n",
              "        '36.187559']], dtype='<U75')"
            ]
          },
          "metadata": {},
          "execution_count": 469
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1wjqra3_yZo",
        "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EX03\n",
        "### Dataframes\n",
        "\n",
        "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
      ],
      "metadata": {
        "id": "SHsrGdy-5AZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "jIatRWhm5Xzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github 101\n",
        "\n",
        "```shell\n",
        "# Repartir sur une nouvelle branche\n",
        "git checkout main\n",
        "# Mettre √† jour notre branche locale\n",
        "git pull origin main\n",
        "# Recr√©er une branche pour le TD 05\n",
        "git checkout -b branch_name_td05\n",
        "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
        "cd 'chemin/m1-miage/'\n",
        "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "# Faire le travail necessaire sur votre fichier et commit et push\n",
        "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "git commit -m 'message descriptif du travail fait'\n",
        "git push --set-upstream origin nom_branche\n",
        "# Faire une pull request\n",
        "```\n",
        "\n",
        "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
      ],
      "metadata": {
        "id": "94-12vgPXG10"
      }
    }
  ]
}